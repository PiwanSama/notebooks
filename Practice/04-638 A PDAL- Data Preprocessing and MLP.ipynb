{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f212e0",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e5434",
   "metadata": {},
   "source": [
    "Data preprocessing: Enables one to improve the quality of training sets.\n",
    "\n",
    "Involves handling missing data, encoding categorical data including categorical classes, partitioning the data into training and test sets, scaling or standardizing the data, and selecting meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eeb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df=pd.read_csv('python-data/heart_failure_clinical_records_dataset.csv')\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bfd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df['time'].sample(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.drop(['time'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ec6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X): #similar to what MinMaxScaler does.\n",
    "    return (X-min(X))/(max(X)-min(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b53d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "age=scale_features(heart_df.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "age[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfa05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(X): #similar to what StandardScaler does.\n",
    "    return (X-X.mean())/X.std()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "age=standardize_features(heart_df.age)\n",
    "age[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(age), np.min(age),np.mean(age),np.var(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=heart_df.iloc[:,:11]\n",
    "y=heart_df.iloc[:,11]\n",
    "X.shape, heart_df.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75751dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy=heart_df.iloc[:,:12]\n",
    "X_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use apply() to standardize the features.\n",
    "X=X.apply(standardize_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e38c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy.apply(scale_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e9b91",
   "metadata": {},
   "source": [
    "# Select meaningful features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b8791",
   "metadata": {},
   "source": [
    "Reducing complexity of models through dimensionality reduction using feature selection\n",
    "a) <b>Feature selection dimensionality reduction</b>: selects a subset of the original features.\n",
    "b) <b>Feature extraction dimensionality reduction</b>: derives information from the original features to create new ones.\n",
    "\n",
    "Consider sequential feature selection algorithms: reduce initial n-dimensional feature space to k-dimensional feature subspace, k<n.\n",
    "\n",
    "Requires features that are most relevant, remove irrelevant features, reduce noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9ada5",
   "metadata": {},
   "source": [
    "##### a) Sequential Backward Selection(SBS)\n",
    "\n",
    "Seeks to reduce dimensionality with minimum classifier performance degradation.\n",
    "Sequentially removes features until the desired features are found. Uses a function to determine\n",
    "the feature to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1003aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of SBS\n",
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SBS():\n",
    "    def __init__(self,estimator,k_features, scoring=accuracy_score, \\\n",
    "                 test_size=0.25, random_state=1):\n",
    "        self.scoring=scoring\n",
    "        self.estimator=clone(estimator) #creates a copy of the estimator with same \n",
    "                                        #parameters but not the fitted data\n",
    "        self.k_features=k_features\n",
    "        self.test_size=test_size\n",
    "        self.random_state=random_state\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        X_train, X_test, y_train, y_test=\\\n",
    "            train_test_split(X,y, test_size=self.test_size,random_state=self.random_state)\n",
    "        dim=X_train.shape[1]\n",
    "        self.indices_=tuple(range(dim))\n",
    "        print(\"---indices\",self.indices_)\n",
    "        print(\"Data::\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        print(\"Data type::\",type(X_train), type(y_train))\n",
    "        self.subsets_=[self.indices_]\n",
    "        print('---subsets_',self.subsets_)\n",
    "        score=self._calc_score(X_train, y_train,X_test, y_test, self.indices_)\n",
    "        self.scores_=[score]\n",
    "        print(\"---scores_:\",self.scores_)\n",
    "        \n",
    "        while dim>self.k_features:\n",
    "            scores=[]\n",
    "            subsets=[]\n",
    "            \n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                print(\"p:\",p)\n",
    "                print(self.indices_)\n",
    "                score=self._calc_score(X_train, y_train,X_test, y_test,p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "                \n",
    "            best=np.argmax(scores)\n",
    "            self.indices_=subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim-=1\n",
    "            \n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_=self.scores_[-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:,self.indices_]\n",
    "    \n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        #self.estimator.fit(X_train, y_train)\n",
    "        self.estimator.fit(X_train[:,indices], y_train)\n",
    "        #y_pred=self.estimator.predict(X_test)\n",
    "        y_pred=self.estimator.predict(X_test[:,indices])\n",
    "        score=self.scoring(y_test,y_pred)\n",
    "        print('---a score::',score)\n",
    "        return score\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.copy(X)\n",
    "y=np.copy(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test=\\\n",
    "            train_test_split(X,y, test_size=0.3,random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "sbs=SBS(knn, k_features=1)\n",
    "sbs.fit(X_train,y_train) #why pass X_train and y_train\n",
    "#we want SBS to create new training subsets for testing (validation) and training.\n",
    "#Approach is used to avoid  original test data from being used as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot classification accuracy of KNN on the validation set.\n",
    "k_feat=[len(k) for k in sbs.subsets_]\n",
    "\n",
    "plt.plot(k_feat, sbs.scores_,marker='+')\n",
    "plt.ylim([0.4,1.0])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which smallest feature set yielded the good scores (k=3)\n",
    "#print(sbs.subsets_)\n",
    "k3=list(sbs.subsets_[8])\n",
    "print(\"sub sets:\",k3)\n",
    "print(heart_df.columns[:-1][k3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the performance of the model on the original test set---this uses all feature sets\n",
    "knn.fit(X_train,y_train)\n",
    "print(\"Training accuracy: \",knn.score(X_train,y_train))\n",
    "print(\"Test accuracy: \",knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce504f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the performance of the model on the original test set---this uses only the three\n",
    "#significant features\n",
    "knn.fit(X_train[:,k3],y_train)\n",
    "print(\"Training accuracy: \",knn.score(X_train[:,k3],y_train))\n",
    "print(\"Test accuracy: \",knn.score(X_test[:,k3],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which smallest feature set yielded the good scores (k=4)\n",
    "#print(sbs.subsets_)\n",
    "k4=list(sbs.subsets_[7])\n",
    "print(\"sub sets:\",k4)\n",
    "print(heart_df.columns[:-1][k4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae57158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the performance of the model on the original test set---this uses only the three\n",
    "#significant features\n",
    "knn.fit(X_train[:,k4],y_train)\n",
    "print(\"Training accuracy: \",knn.score(X_train[:,k4],y_train))\n",
    "print(\"Test accuracy: \",knn.score(X_test[:,k4],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb27c3",
   "metadata": {},
   "source": [
    "<b>Observation</b>: Clearly the performance is better with reduced dataset from our heart failure dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6f41a",
   "metadata": {},
   "source": [
    "# Assessing feature importance with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be585a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_labels=heart_df.columns[:-1]\n",
    "rf=RandomForestClassifier(n_estimators=500,random_state=1)\n",
    "rf.fit(X_train,y_train)\n",
    "importances=rf.feature_importances_\n",
    "indices=np.argsort(importances)[::-1]\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"{:2d} {:25s} {:.3f}\".format(f+1, feat_labels[indices[f]], importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X_train.shape[1]),feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1,X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38393c3e",
   "metadata": {},
   "source": [
    "<b>Observation</b>: ejection_fraction, and serum_creatinine are the top two predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8183739",
   "metadata": {},
   "source": [
    "# Select features with SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d14b6",
   "metadata": {},
   "source": [
    "Useful when we need to use a Pipeline object. Based on a predefined threshold set by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sfm=SelectFromModel(rf,threshold=0.15, prefit=True)\n",
    "X_selected=sfm.transform(X_train)\n",
    "X_test_selected=sfm.transform(X_test)\n",
    "print(\"Number of samples that meet this criterion: \", X_selected.shape[0])\n",
    "\n",
    "for f in range(X_selected.shape[1]):\n",
    "    print(\"{:2d} {:25s} {:.3f}\".format(f+1, feat_labels[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a11f88",
   "metadata": {},
   "source": [
    "More on feature selction: https://scikit-learn.org/stable/modules/feature_selection.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a839b",
   "metadata": {},
   "source": [
    "# Debugging Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb448d7",
   "metadata": {},
   "source": [
    "Uses <b>learning</b> and <b>validation</b> curves.\n",
    "\n",
    "Diagnose <b>high variance (overfitting)</b> or <b>high bias (underfitting)</b> using <b>learning curves</b>.\n",
    "\n",
    "Diagnose common issues with learning algorithms using <b>validation curves</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed281bda",
   "metadata": {},
   "source": [
    "# a) Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7201364",
   "metadata": {},
   "source": [
    "Plots model training and validation accuracies as functions of the training set size.\n",
    "\n",
    "Model with high bias (underfitting): Low training and cross-validation accuracy. Can be addressed by increasing the number of model parameters/features (either by more data collection or feature construction) or decreasing the degree of regularization.\n",
    "\n",
    "Model with high variance(overfitting): Large gap between training and cross-validation accuracy. May be addressd by collecting more training data, reducing model complexity, or increasing the regularization parameter (for regularized models). You may reduce the number of features through feature selection or feature extraction to decrease overfitting for unegularized models.  Careful if the training examples are very noisy...in that case you may also need to reduce the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde32923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve, StratifiedKFold,cross_val_score\n",
    "\n",
    "lr_model=LogisticRegression(random_state=1)\n",
    "#learning_curve uses stratified k-fold cross validation\n",
    "train_sizes, train_scores, test_scores=learning_curve(estimator=lr_model, X=X_train, y=y_train,\\\n",
    "                                                     train_sizes=np.linspace(0.1,1.0,10),cv=10,n_jobs=1)\n",
    "train_mean=np.mean(train_scores,axis=1)\n",
    "train_std=np.std(train_scores,axis=1) #used to indicate the variance of estimate.\n",
    "\n",
    "test_mean=np.mean(test_scores,axis=1)\n",
    "test_std=np.std(test_scores,axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='red', marker='+', markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes, train_mean+train_std, train_mean-train_std, alpha=0.15, color='red')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes, test_mean+test_std, test_mean-test_std, alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Learning curves\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.5,1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc002a4a",
   "metadata": {},
   "source": [
    "Question: How well does our model perform?\n",
    "\n",
    "Good performance from around 100 samples. With fewer examples we have overfitting--the gap between training and validation accuracy increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1ac4a",
   "metadata": {},
   "source": [
    "# b) Validation curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4567d83",
   "metadata": {},
   "source": [
    "Used to improve model performance by addressing overfitting and underfitting.\n",
    "\n",
    "Plots accuracy against variations in a model parameter.\n",
    "\n",
    "Consider varying the regularization parameter c in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b970159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range=[0.001, 0.01, 0.1,1.0,10.0, 100.0]\n",
    "lr_model2=LogisticRegression(random_state=1)\n",
    "\n",
    "print(\"Params::\",lr_model2.get_params().keys())\n",
    "#Uses stratified k-fold cross-validation.\n",
    "train_scores, test_scores=validation_curve(estimator=lr_model2, X=X_train, y=y_train,\\\n",
    "                                                     param_name='C', \\\n",
    "                                         param_range=param_range,cv=10)\n",
    "train_mean=np.mean(train_scores,axis=1)\n",
    "train_std=np.std(train_scores,axis=1) #used to indicate the variance of estimate.\n",
    "\n",
    "test_mean=np.mean(test_scores,axis=1)\n",
    "test_std=np.std(test_scores,axis=1)\n",
    "\n",
    "\n",
    "plt.plot(param_range, train_mean, color='red', marker='+', markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(param_range, train_mean+train_std, train_mean-train_std, alpha=0.15, color='red')\n",
    "\n",
    "plt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "\n",
    "plt.fill_between(param_range, test_mean+test_std, test_mean-test_std, alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Validation curves---parameter:C\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.65,0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27486f5",
   "metadata": {},
   "source": [
    "Observation: Towards c=100, there seems to be overfitting. Between c=0.001 to c=0.01 seems the better range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "lr_model2=LogisticRegression(random_state=1)\n",
    "\n",
    "print(\"Params::\",lr_model2.get_params().keys())\n",
    "#Uses stratified k-fold cross-validation.\n",
    "train_scores, test_scores=validation_curve(estimator=lr_model2, X=X_train, y=y_train,\\\n",
    "                                                     param_name='solver', \\\n",
    "                                         param_range=param_range,cv=10)\n",
    "train_mean=np.mean(train_scores,axis=1)\n",
    "train_std=np.std(train_scores,axis=1) #used to indicate the variance of estimate.\n",
    "\n",
    "test_mean=np.mean(test_scores,axis=1)\n",
    "test_std=np.std(test_scores,axis=1)\n",
    "\n",
    "\n",
    "plt.plot(param_range, train_mean, color='red', marker='x', markersize=6, label='training accuracy')\n",
    "\n",
    "plt.fill_between(param_range, train_mean+train_std, train_mean-train_std, alpha=0.15, color='red')\n",
    "\n",
    "plt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=6, label='validation accuracy')\n",
    "\n",
    "plt.fill_between(param_range, test_mean+test_std, test_mean-test_std, alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('solver')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Validation curves:::for solver parameter\")\n",
    "plt.xticks(range(len(param_range)),param_range, rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14aa47",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron: With or without feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119773e",
   "metadata": {},
   "source": [
    "#### Initial Model with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Model with all the features\n",
    "mlp = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10, 4), \\\n",
    "                    solver='lbfgs', activation='relu', batch_size=32)\n",
    "#solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "#activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "\n",
    "#Working with the original dataset after scaling\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions=mlp.predict(X_test)\n",
    "score=mlp.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70bacb",
   "metadata": {},
   "source": [
    "#### Another Model with Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d015bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with selected features\n",
    "mlp = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10,4), solver='lbfgs', activation='relu', batch_size=32)\n",
    "#solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "#activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "\n",
    "#Working with selected features.\n",
    "mlp.fit(X_selected, y_train)\n",
    "predictions=mlp.predict(X_test_selected)\n",
    "score=mlp.score(X_test_selected, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e6460",
   "metadata": {},
   "source": [
    "# Other Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd050f",
   "metadata": {},
   "source": [
    "Remove features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.65 * (1 - .65)))\n",
    "X_train_var_thresh=sel.fit_transform(X_train)\n",
    "X_test_var_thresh=sel.transform(X_test)\n",
    "X_train_var_thresh.shape, X_test_var_thresh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcab8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10, 4), solver='lbfgs', activation='relu', batch_size=32)\n",
    "#solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "#activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "\n",
    "#Working with selected features.\n",
    "mlp.fit(X_train_var_thresh, y_train)\n",
    "predictions=mlp.predict(X_test_var_thresh)\n",
    "score=mlp.score(X_test_var_thresh, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff685c2e",
   "metadata": {},
   "source": [
    "# Compare Algorithms in Batch Mode \n",
    "This aims to identify promising candidate algorithms that should be explored further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pycaret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e76497",
   "metadata": {},
   "source": [
    "# Implementing Perceptron and ADALINE learning strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651d92e",
   "metadata": {},
   "source": [
    "The following examples  have been derived(with little adjustment) from:\n",
    "\n",
    "Sebastian Raschka & Vahid Mirjalili (2017), Python Machine Learning, 2nd Edition- Machine Learning and Deep Learning with Python,scikit-learn, and TensorFlow. Packt.\n",
    "\n",
    "Link: https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019579188304436"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbd28e",
   "metadata": {},
   "source": [
    "# Implementing Perceptron classifier---problem should be linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    '''perceptron classifier\n",
    "    \n",
    "    paremeters:#set at the start\n",
    "    miu: float (ranges 0 to 1)---learning rate\n",
    "    n_iter: int (number of iterations aka epochs)---how many times to pass through the dataset. NUmber of epochs.\n",
    "    random_state: int (random number generator seed for random weight initialization)\n",
    "    \n",
    "    attributes: #set at training\n",
    "    w_: 1d-array (weights after fitting)\n",
    "    errors_: list (number of misclassifications (updates) in each epoch)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, miu=0.01,n_iter=5, random_state=1):\n",
    "        self.miu=miu\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Learn from the training data\n",
    "        parameters:\n",
    "        X:{array_like}, shape={n_samples, n_features}--training vectors\n",
    "        y:array-like, shape={n_samples}---target values---correct/ground truth values\n",
    "        \n",
    "        returns:\n",
    "        self:object\n",
    "        '''\n",
    "        \n",
    "        rgen=np.random.RandomState(self.random_state) #seeding allows producing previous results if needed.\n",
    "        self.w_=rgen.normal(loc=0.0, scale=0.01, size=1+X.shape[1]) #we just want small random values.\n",
    "                    #weights from normal distribution with stdev=0.01\n",
    "        print(\"----weights----\", self.w_)\n",
    "        self.errors_=[]\n",
    "        \n",
    "        for _ in  range(self.n_iter):\n",
    "            errors=0\n",
    "            for xi, target in zip(X,y):\n",
    "                update=self.miu*(target-self.predict(xi))\n",
    "                self.w_[1:]+=update*xi\n",
    "                \n",
    "                self.w_[0]+=update*1  #self.w_[0] is weight of bias unit.\n",
    "                \n",
    "                errors+=int(update!=0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self,X): #result of wTx\n",
    "        '''Calculate net input'''\n",
    "        return np.dot(X,self.w_[1:])+self.w_[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Return class label after unit step'''\n",
    "        return np.where(self.net_input(X)>=0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two class classification---setosa and versicolor.\n",
    "df=pd.read_csv('python-data/iris_dataset.csv')\n",
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the first 100 instances---the setosas and versicolors\n",
    "#convert the classes to -1(setosa), 1 (versicolor)\n",
    "y=df.iloc[0:100,4].values\n",
    "y=np.where(y=='Iris-setosa',-1,1)\n",
    "\n",
    "#extract features\n",
    "X=df.iloc[0:100,0:2].values\n",
    "\n",
    "#plot the data for exploration\n",
    "plt.scatter(X[:50,0], X[:50,1], color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100,0], X[50:100,1], color='blue', marker='+', label='versicolor')\n",
    "plt.xlabel('sepal length(cm)')\n",
    "plt.ylabel('sepal width(cm)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01986655",
   "metadata": {},
   "source": [
    "See the above plot shows that the data is linearly separable for two columns. Therefore, we can use a linear classifier like the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff236078",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dedd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76065dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's train the perceptron\n",
    "ppn=Perceptron(miu=0.1, n_iter=150)\n",
    "ppn.fit(X,y)\n",
    "plt.plot(range(1,len(ppn.errors_)+1),ppn.errors_, marker='+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the decision boundary\n",
    "def plot_decision_regions(X,y,classifier, step=0.01):\n",
    "    #set up marker generator and color map\n",
    "    markers=['s','x','o','^','v']\n",
    "    colors=['red','blue', 'lightgreen','gray','cyan']\n",
    "    cmap=ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    #plot the decision surface\n",
    "    x1_min,x1_max=X[:,0].min()-1,X[:,0].max()+1\n",
    "    x2_min,x2_max=X[:,1].min()-1,X[:,1].max()+1\n",
    "    \n",
    "    xx1,xx2=np.meshgrid(np.arange(x1_min,x1_max,step),np.arange(x2_min,x2_max,step))\n",
    "    Z=classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T) #ravel flattens the array\n",
    "    Z=Z.reshape(xx1.shape)\n",
    "    \n",
    "    plt.contourf(xx1,xx2,Z,alpha=0.1, cmap=cmap) #alpha: between 0(transparent) and 1(opaque)--change it and see the variations\n",
    "    plt.xlim(xx1.min(),xx1.max())\n",
    "    plt.ylim(xx2.min(),xx2.max())\n",
    "    \n",
    "    #Plot class samples\n",
    "    for idx,cl in enumerate(np.unique(y)): #idx: index, cl: class (setosa, versicolor, etc)\n",
    "        plt.scatter(x=X[y==cl,0], y=X[y==cl,1], \\\n",
    "                    alpha=0.8, c=colors[idx], label=cl, edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f330bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X,y,classifier=ppn)\n",
    "plt.xlabel('sepal length(cm)')\n",
    "plt.ylabel('sepal width(cm)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1bf25",
   "metadata": {},
   "source": [
    "# Implementing ADAptive LInear NEuron(ADALINE) using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5430ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    '''ADAptive LInear NEuron classifier\n",
    "    \n",
    "    This solution uses batch gradient descent.\n",
    "    \n",
    "    paremeters:#set at the start\n",
    "    miu: float (ranges 0 to 1)---learning rate\n",
    "    \n",
    "    n_iter: int (number of iterations aka epochs)---how many times to pass through the dataset. NUmber of epochs.\n",
    "    \n",
    "    random_state: int (random number generator seed for random weight initialization)\n",
    "    \n",
    "    attributes: #set at training\n",
    "    w_: 1d-array (weights after fitting)\n",
    "    \n",
    "    cost_: list (sum of squares cost function value in each epoch)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, miu=0.01,n_iter=5, random_state=1):\n",
    "        self.miu=miu\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Learn from the training data\n",
    "        parameters:\n",
    "        X:{array_like}, shape={n_samples, n_features}--training vectors\n",
    "        y:array-like, shape={n_samples}---target values---correct/ground truth values\n",
    "        \n",
    "        returns:\n",
    "        self:object\n",
    "        '''\n",
    "        \n",
    "        rgen=np.random.RandomState(self.random_state) #seeding allows producing previous results if needed.\n",
    "        self.w_=rgen.normal(loc=0.0, scale=0.01, size=1+X.shape[1]) #we just want small random values.\n",
    "                    #weights from normal distribution with stdev=0.01\n",
    "        self.cost_=[]\n",
    "        \n",
    "        for _ in  range(self.n_iter):\n",
    "            net_input=self.net_input(X)\n",
    "            output=self.activation(net_input)\n",
    "            errors=(y-output) \n",
    "            #print(errors)\n",
    "            #print( \"----output: \",output,\"----errors: \",errors)\n",
    "            self.w_[1:]+=self.miu*X.T.dot(errors)\n",
    "            \n",
    "            self.w_[0]+=self.miu*errors.sum() #self.w_[0] is weight of bias unit.\n",
    "            \n",
    "            cost=(errors**2).sum()/2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self,X): #result of wTx---the summation\n",
    "        '''Calculate net input'''\n",
    "        return np.dot(X,self.w_[1:])+self.w_[0]\n",
    "    \n",
    "    def activation(self,X): #uses activation function, e.g. linear, sigmoid, relu, tanh,etc.\n",
    "        '''Calculate linear activation'''\n",
    "        return X #identity\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Return class label after unit step'''\n",
    "        return np.where(self.activation(self.net_input(X))>=0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting for different learning rates\n",
    "fig,ax=plt.subplots(nrows=1,ncols=2, figsize=(10,4))\n",
    "\n",
    "ada1=AdalineGD(n_iter=15, miu=0.01).fit(X,y)\n",
    "ax[0].plot(range(1,len(ada1.cost_)+1),np.log10(ada1.cost_), marker='x') #len(ada1.cost_)--same as number of epochs\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('(a) Adaline-Learning rate=0.01')\n",
    "\n",
    "ada2=AdalineGD(n_iter=15, miu=0.0001).fit(X,y)\n",
    "ax[1].plot(range(1,len(ada2.cost_)+1),np.log10(ada2.cost_), marker='x')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('log(Sum-squared-error)')\n",
    "ax[1].set_title('(b) Adaline-Learning rate=0.0001')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ab7a7",
   "metadata": {},
   "source": [
    "#In a) the errors are increasing because the learning rate is so large that it potentially overshoots the global minimum.\n",
    "#In b), the learning rate is so small that it may requires so many epochs to converge to the global cost minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab278723",
   "metadata": {},
   "source": [
    "#What happens if we scale the data? Let's use standardization x_new=(x_old-mean(x_values))/sdv(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std=np.copy(X) #copy the X values\n",
    "X_std[:,0]=(X[:,0]-X[:,0].mean())/X[:,0].std() #this is what StandardScaler does\n",
    "X_std[:,1]=(X[:,1]-X[:,1].mean())/X[:,1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std[:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting for different learning rates\n",
    "fig,ax=plt.subplots(nrows=1,ncols=2, figsize=(10,4))\n",
    "\n",
    "ada1=AdalineGD(n_iter=15, miu=0.01).fit(X_std,y)\n",
    "ax[0].plot(range(1,len(ada1.cost_)+1),np.log10(ada1.cost_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('(a) Adaline-Learning rate=0.01')\n",
    "\n",
    "ada2=AdalineGD(n_iter=15, miu=0.0001).fit(X_std,y)\n",
    "ax[1].plot(range(1,len(ada2.cost_)+1),np.log10(ada2.cost_), marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('log(Sum-squared-error)')\n",
    "ax[1].set_title('(b) Adaline-Learning rate=0.0001')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c05367",
   "metadata": {},
   "source": [
    "#See the behavior of a) and b). Also notice that standardized values have helped with the early convergence with 0.01 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7af294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's plot the decision boundary\n",
    "ada=AdalineGD(n_iter=15, miu=0.01).fit(X_std,y)\n",
    "\n",
    "plot_decision_regions(X_std,y,classifier=ada)\n",
    "plt.xlabel('sepal length(cm)')\n",
    "plt.ylabel('sepal width(cm)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59fc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the errors per epoch\n",
    "plt.plot(range(1,len(ada.cost_)+1), ada.cost_,marker='s')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sum-squared-error')\n",
    "plt.title('Adaline-Learning rate=0.01, standardized data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba82496",
   "metadata": {},
   "source": [
    "# Implementing ADAptive LInear NEuron(ADALINE) using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19dbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD(object):\n",
    "    '''ADAptive LInear NEuron classifier\n",
    "    \n",
    "    This solution uses stochastic gradient descent.\n",
    "    \n",
    "    paremeters:#set at the start\n",
    "    miu: float (ranges 0 to 1)---learning rate\n",
    "    \n",
    "    n_iter: int (number of iterations aka epochs)---how many times to pass through the dataset. NUmber of epochs.\n",
    "    \n",
    "    random_state: int (random number generator seed for random weight initialization)\n",
    "    \n",
    "    shuffle: bool (default:True) (shuffles the the training data every epoch if True) to prevent cycles.\n",
    "    \n",
    "    attributes: #set at training\n",
    "    w_: 1d-array (weights after fitting)\n",
    "    \n",
    "    cost_: list (sum of squares cost function value averaged over all training samples in each epoch)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, miu=0.01,n_iter=5, shuffle=True, random_state=1):\n",
    "        self.miu=miu\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "        self.shuffle=shuffle\n",
    "        self.w_initialized=False\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Learn from the training data\n",
    "        parameters:\n",
    "        X:{array_like}, shape={n_samples, n_features}--training vectors\n",
    "        y:array-like, shape={n_samples}---target values---correct/ground truth values\n",
    "        \n",
    "        returns:\n",
    "        self:object\n",
    "        '''\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_=[]\n",
    "        \n",
    "        for _ in  range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X,y=self._shuffle(X,y)\n",
    "            cost=[]\n",
    "            for xi, target in zip(X,y):\n",
    "                cost.append(self._update_weights(xi,target))\n",
    "            avg_cost=sum(cost)/len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        '''\n",
    "        Can be used in online learning scenarios with streaming data.\n",
    "        \n",
    "        Fits data without re-initializing the weights.\n",
    "        \n",
    "        Learn from the training data\n",
    "        \n",
    "        parameters:\n",
    "        X:{array_like}, shape={n_samples, n_features}--training vectors\n",
    "        y:array-like, shape={n_samples}---target values---correct/ground truth values\n",
    "        \n",
    "        returns:\n",
    "        self:object\n",
    "        '''\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0]>1:\n",
    "            for xi,target in zip(X,y):\n",
    "                self._update_weights(xi,target)\n",
    "                \n",
    "        else:\n",
    "            self._update_weights(X,y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _shuffle(self,X,y):\n",
    "        '''\n",
    "        Shuffles the training data\n",
    "        '''\n",
    "        r=self.rgen.permutation(len(y))\n",
    "        return X[r],y[r]\n",
    "        \n",
    "        \n",
    "    def _initialize_weights(self,m):\n",
    "        '''\n",
    "        Initializes weights to small random numbers\n",
    "        '''\n",
    "        self.rgen=np.random.RandomState(self.random_state) #seeding allows producing previous results if needed.\n",
    "        self.w_=self.rgen.normal(loc=0.0, scale=0.01, size=1+m) #we just want small random values.\n",
    "                    #weights from normal distribution with stdev=0.01\n",
    "        self.w_initialized=True\n",
    "    \n",
    "    def _update_weights(self,xi,target):\n",
    "        '''\n",
    "        Apply Adaline learning rule to update the weights.\n",
    "        '''\n",
    "        output=self.activation(self.net_input(xi))\n",
    "        error=(target-output) \n",
    "        print(\"----xi: \",xi, \"----output: \",output,\"----error: \",error)\n",
    "        self.w_[1:]+=self.miu*xi.dot(error)\n",
    "        self.w_[0]+=self.miu*error #self.w_[0] is weight of bias unit.\n",
    "        cost=(error**2)/2.0\n",
    "        return cost\n",
    "       \n",
    "    def net_input(self,X): #result of wTx---the summation\n",
    "        '''Calculate net input'''\n",
    "        return np.dot(X,self.w_[1:])+self.w_[0]\n",
    "    \n",
    "    def activation(self,X): #uses activation function\n",
    "        '''Calculate linear activation'''\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Return class label after unit step'''\n",
    "        return np.where(self.activation(self.net_input(X))>=0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "adaSGD=AdalineSGD(n_iter=15, miu=0.01, random_state=1)\n",
    "adaSGD.fit(X_std,y)\n",
    "\n",
    "#plot the decision boundary\n",
    "plot_decision_regions(X_std,y,classifier=adaSGD)\n",
    "plt.xlabel('sepal length(cm)')\n",
    "plt.ylabel('sepal width(cm)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Adaline SGD--standardized data')\n",
    "plt.show()\n",
    "\n",
    "#Plot the errors per epoch\n",
    "plt.plot(range(1,len(ada.cost_)+1), ada.cost_,marker='+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('average cost')\n",
    "plt.title('Adaline SGD: Learning rate=0.01, standardized data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61042ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaSGD.partial_fit(X_std[0,:],y[0]) #Online learning demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad68464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
