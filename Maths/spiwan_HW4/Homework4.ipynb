{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Real estate.csv\")\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "df.drop(\"No\", axis = 1, inplace = True)\n",
    "column_maping = {}\n",
    "for i in df.columns:\n",
    "    new_column = i[3:].capitalize().replace(' ', '_')\n",
    "    column_maping[i] = new_column\n",
    "# Now we will rename the column using the dictinary \n",
    "df.rename(columns = column_maping, inplace = True)\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "X  = df.drop(['Transaction_date', \"House_price_of_unit_area\"], axis = 1)\n",
    "y = df['House_price_of_unit_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>House_age</th>\n",
       "      <th>Distance_to_the_nearest_mrt_station</th>\n",
       "      <th>Number_of_convenience_stores</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>13.7</td>\n",
       "      <td>4082.01500</td>\n",
       "      <td>0</td>\n",
       "      <td>24.94155</td>\n",
       "      <td>121.50381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>5.6</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>24.97433</td>\n",
       "      <td>121.54310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>18.8</td>\n",
       "      <td>390.96960</td>\n",
       "      <td>7</td>\n",
       "      <td>24.97923</td>\n",
       "      <td>121.53986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>8.1</td>\n",
       "      <td>104.81010</td>\n",
       "      <td>5</td>\n",
       "      <td>24.96674</td>\n",
       "      <td>121.54067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>6.5</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>24.97433</td>\n",
       "      <td>121.54310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     House_age  Distance_to_the_nearest_mrt_station  \\\n",
       "0         32.0                             84.87882   \n",
       "1         19.5                            306.59470   \n",
       "2         13.3                            561.98450   \n",
       "3         13.3                            561.98450   \n",
       "4          5.0                            390.56840   \n",
       "..         ...                                  ...   \n",
       "409       13.7                           4082.01500   \n",
       "410        5.6                             90.45606   \n",
       "411       18.8                            390.96960   \n",
       "412        8.1                            104.81010   \n",
       "413        6.5                             90.45606   \n",
       "\n",
       "     Number_of_convenience_stores  Latitude  Longitude  \n",
       "0                              10  24.98298  121.54024  \n",
       "1                               9  24.98034  121.53951  \n",
       "2                               5  24.98746  121.54391  \n",
       "3                               5  24.98746  121.54391  \n",
       "4                               5  24.97937  121.54245  \n",
       "..                            ...       ...        ...  \n",
       "409                             0  24.94155  121.50381  \n",
       "410                             9  24.97433  121.54310  \n",
       "411                             7  24.97923  121.53986  \n",
       "412                             5  24.96674  121.54067  \n",
       "413                             9  24.97433  121.54310  \n",
       "\n",
       "[414 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error using SVD 7.38789179677546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "# Do not change the code Above\n",
    "\n",
    "# Add a column of ones to X_train and X_test to account for the bias term (intercept)\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Perform SVD decomposition on the training data\n",
    "U, s, VT = np.linalg.svd(X_train_b, full_matrices=False)\n",
    "\n",
    "# Create diagonal matrix for Sigma\n",
    "S_diag = np.diag(s)\n",
    "\n",
    "# Compute the pseudo-inverse of the training data\n",
    "X_train_pinv = VT.T @ np.linalg.inv(S_diag) @ U.T\n",
    "\n",
    "# Calculate the weights (regression coefficients), including the bias term (intercept)\n",
    "w = X_train_pinv @ y_train\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = X_test_b @ w\n",
    "\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error using SVD {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error Ridge 7.387798950430715\n"
     ]
    }
   ],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "     Ridge Regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, alpha=0):\n",
    "        \"\"\"\n",
    "        Fits the ridge regression model to the training data.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p independent variables\n",
    "        y: response variable vector for n examples\n",
    "        alpha: regularization parameter.\n",
    "        \"\"\"\n",
    "      \n",
    "        intercept = np.ones((len(X),1))\n",
    "        X_b = np.c_[intercept,X]\n",
    "        \n",
    "        I = np.identity(X_b.shape[1])\n",
    "        \n",
    "        betha_optim = ##TODO What is the optimal Beta you solved ?\n",
    "        self.betas = betha_optim\n",
    "        return betha_optim\n",
    "   \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the dependent variable of new data using the model.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        response variable vector for n examples\n",
    "        \"\"\"\n",
    "       \t# Your code here\n",
    "        X_predictor = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.predictions = X_predictor @ self.betha_optim\n",
    "        return self.predictions\n",
    "\n",
    "    def rmse(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the RMSE(Root Mean Squared Error) when the model is validated.\n",
    "            \n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "        y: response variable vector for n examples\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        RMSE when model is used to predict y\n",
    "        \"\"\"\n",
    "        y_predict = self.predict(X=X)\n",
    "        se = (y_predict-y) ** 2\n",
    "        mse = se.mean()\n",
    "        rmse = mse**0.5\n",
    "        return rmse\n",
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "my_model = Model()\n",
    "my_model.fit(X=X_train, y=y_train, alpha=0.01)\n",
    "rmse = my_model.rmse(X=X_test, y=y_test)\n",
    "print(f\"Root Mean Squre Error Ridge {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 1)\n",
      "Root Mean Squre Error 1 Layer MLP 15.69657819926082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# Mean Squared Error loss\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_o, b_o):\n",
    "    # The output of the hidden layer\n",
    "    a_h1 = X@W_h1 + b_h1\n",
    "    # Apply activation to the output of the hidden layer\n",
    "    z_h1 = sigmoid(a_h1)\n",
    "    # Use ouput of activation as input to the output layer (it is just similar to first layer but we don't apply activation)\n",
    "    y_pred = z_h1@W_o + b_o\n",
    "    return y_pred, z_h1, a_h1\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, W_h1, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred\n",
    "    dL_dy_pred = #TODO What type of loss function are we using, what is it's derivative ?\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dL_dW_o = #TODO remember the output layer is just a dense layer\n",
    "    dL_db_o = #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    dL_dz_h1 = #TODO derivative with respect to the output of activation layer\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    dL_da_h1 = #TODO derivative with respect to the activation layer\n",
    "    \n",
    "    # Gradients for hidden layer\n",
    "    dL_dW_h1 = #TODO\n",
    "    dL_db_h1 = #TODO \n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer_size = 20 # Number of neurons in layer\n",
    "output_size = 1 # predicted variable\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer_size))\n",
    "W_o = np.random.randn(hidden_layer_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "#To save the weights which give the lowest loss\n",
    "lowest_loss = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1 = forward_pass(X_train, W_h1, b_h1, W_o, b_o)\n",
    "    loss = mse_loss(y_train, y_pred)\n",
    "\n",
    "    if loss < lowest_loss:\n",
    "        lowest_loss = loss\n",
    "        # Save the best weights and biases\n",
    "        best_weights = (W_h1.copy(), b_h1.copy(), W_o.copy(), b_o.copy())\n",
    "\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, W_h1, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases\n",
    "    W_h1 -= #TODO\n",
    "    b_h1 -= #TODO\n",
    "    W_o -= #TODO\n",
    "    b_o -= #TODO\n",
    "\n",
    "\n",
    "W_h1_best, b_h1_best, W_o_best, b_o_best = best_weights\n",
    "y_pred_test, _, _ = forward_pass(X_test, W_h1_best, b_h1_best, W_o_best, b_o_best)\n",
    "se = (y_pred_test-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 1 Layer MLP {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Lets add one more hidden layer (10 Points) \n",
    "### You must write down all gradients and complete the code below to get full bonus points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error 1 Layer MLP 13.11351371204041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_h2, b_h2, W_o, b_o):\n",
    "    a_h1 = #TODO compute ouput of first hidden layer\n",
    "    z_h1 = sigmoid(a_h1) # apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    a_h2 = #TODO compute ouput of second hidden layer\n",
    "    z_h2 = sigmoid(a_h2) #apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    y_pred = # compute ouput of output layer, why don't we apply activation ?\n",
    "    \n",
    "    return y_pred, z_h1, a_h1, z_h2, a_h2\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred, what kind of loss are we using ?\n",
    "    dL_dy_pred = #TODO\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dL_dW_o = #TODO\n",
    "    dL_db_o = #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h2\n",
    "    dL_dz_h2 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to a_h2\n",
    "    dL_da_h2 =  #TODO\n",
    "    \n",
    "    # Gradients for second hidden layer\n",
    "    dL_dW_h2 =  #TODO\n",
    "    dL_db_h2 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    dL_dz_h1 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    dL_da_h1 =  #TODO\n",
    "    \n",
    "    # Gradients for first hidden layer\n",
    "    dL_dW_h1 =  #TODO\n",
    "    dL_db_h1 =  #TODO\n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "# Random input and true output (modify these according to your dataset)\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer1_size = 100\n",
    "hidden_layer2_size = 20\n",
    "output_size = 1\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer1_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer1_size))\n",
    "W_h2 = np.random.randn(hidden_layer1_size, hidden_layer2_size) * 0.001\n",
    "b_h2 = np.zeros((1, hidden_layer2_size))\n",
    "W_o = np.random.randn(hidden_layer2_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Training loop\n",
    "for i in range(200):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1, z_h2, a_h2 = forward_pass(X_train, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "\n",
    "    #TODO Compute the loss\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases for each layer\n",
    "    \n",
    "    W_h1 -= #TODO\n",
    "    b_h1 -= #TODO\n",
    "    W_h2 -= #TODO\n",
    "    b_h2 -= #TODO\n",
    "    W_o -= #TODO\n",
    "    b_o -= #TODO\n",
    "\n",
    "y_pred, _, _, _, _ = forward_pass(X_test, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 2 Layer MLP {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 400)               2400      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 400)               160400    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 400)               160400    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 401       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 323601 (1.23 MB)\n",
      "Trainable params: 323601 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform',  activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform', activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(400, kernel_initializer = 'he_uniform',activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 1s 12ms/step - loss: 59.0233 - val_loss: 38.2307\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.5859 - val_loss: 38.7539\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5126 - val_loss: 39.4410\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.5206 - val_loss: 41.2996\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.8750 - val_loss: 36.7995\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.7921 - val_loss: 42.3474\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.6418 - val_loss: 38.9120\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.3855 - val_loss: 34.8828\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5909 - val_loss: 38.9493\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3074 - val_loss: 46.1832\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3386 - val_loss: 48.0670\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 65.7009 - val_loss: 47.4946\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 61.9314 - val_loss: 36.4857\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.1176 - val_loss: 39.1615\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.7488 - val_loss: 45.9356\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1246 - val_loss: 42.3506\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.2204 - val_loss: 38.4740\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.5598 - val_loss: 40.0011\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.5267 - val_loss: 41.0102\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.7600 - val_loss: 41.7729\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 65.8730 - val_loss: 40.3871\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.0501 - val_loss: 39.3643\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.8604 - val_loss: 39.8595\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 61.1329 - val_loss: 38.9457\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.6057 - val_loss: 41.5118\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.1321 - val_loss: 35.7910\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1891 - val_loss: 39.2262\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3964 - val_loss: 36.2836\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.9470 - val_loss: 45.4145\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.6735 - val_loss: 37.7727\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.3257 - val_loss: 48.5326\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 64.4544 - val_loss: 43.9641\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.6239 - val_loss: 46.4309\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 58.7664 - val_loss: 38.9237\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.0663 - val_loss: 40.5066\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 54.7116 - val_loss: 43.4164\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.9722 - val_loss: 44.4749\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.3571 - val_loss: 47.2762\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.0204 - val_loss: 53.4110\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5805 - val_loss: 40.7375\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.9992 - val_loss: 40.2645\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1041 - val_loss: 42.0701\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.9374 - val_loss: 46.8338\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 54.1358 - val_loss: 45.7144\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.6320 - val_loss: 39.5554\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 71.7491 - val_loss: 54.7381\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.1944 - val_loss: 45.8185\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 59.1702 - val_loss: 45.4068\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "history = model.fit(X_train, y_train,  epochs = 200, validation_data=(X_test, y_test), \n",
    "                    callbacks = EarlyStopping(monitor = 'val_loss',patience = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 45.4068\n",
      "RMSE error using deeper neural network 6.738456462848816\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE error using deeper neural network {model.evaluate(X_test, y_test)**0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
