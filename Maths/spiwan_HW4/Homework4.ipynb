{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Real estate.csv\")\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "df.drop(\"No\", axis = 1, inplace = True)\n",
    "column_maping = {}\n",
    "for i in df.columns:\n",
    "    new_column = i[3:].capitalize().replace(' ', '_')\n",
    "    column_maping[i] = new_column\n",
    "# Now we will rename the column using the dictinary \n",
    "df.rename(columns = column_maping, inplace = True)\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "X  = df.drop(['Transaction_date', \"House_price_of_unit_area\"], axis = 1)\n",
    "y = df['House_price_of_unit_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error using SVD 7.38789179677546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "# Do not change the code Above\n",
    "\n",
    "# Add a column of ones to X_train and X_test to account for the bias term (intercept)\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Perform SVD decomposition on the training data\n",
    "U, s, VT = np.linalg.svd(X_train_b, full_matrices=False)\n",
    "\n",
    "# Create diagonal matrix for Sigma\n",
    "S_diag = np.diag(s)\n",
    "\n",
    "# Compute the pseudo-inverse of the training data\n",
    "X_train_pinv = VT.T @ np.linalg.inv(S_diag) @ U.T\n",
    "\n",
    "# Calculate the weights (regression coefficients), including the bias term (intercept)\n",
    "w = X_train_pinv @ y_train\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = X_test_b @ w\n",
    "\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error using SVD {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error Ridge 7.387798950430713\n"
     ]
    }
   ],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "     Ridge Regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, alpha=0):\n",
    "        \"\"\"\n",
    "        Fits the ridge regression model to the training data.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p independent variables\n",
    "        y: response variable vector for n examples\n",
    "        alpha: regularization parameter.\n",
    "        \"\"\"\n",
    "      \n",
    "        intercept = np.ones((len(X),1))\n",
    "        X_b = np.c_[intercept,X]\n",
    "        \n",
    "        I = np.identity(X_b.shape[1])\n",
    "        \n",
    "        betha_optim = np.linalg.inv(X_b.T@X_b + alpha*I) @ np.dot(X_b.T,y)\n",
    "        self.betas = betha_optim\n",
    "        return betha_optim\n",
    "   \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the dependent variable of new data using the model.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        response variable vector for n examples\n",
    "        \"\"\"\n",
    "       \t# Your code here\n",
    "        X_predictor = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.predictions = X_predictor @ self.betas\n",
    "        return self.predictions\n",
    "\n",
    "    def rmse(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the RMSE(Root Mean Squared Error) when the model is validated.\n",
    "            \n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "        y: response variable vector for n examples\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        RMSE when model is used to predict y\n",
    "        \"\"\"\n",
    "        y_predict = self.predict(X=X)\n",
    "        se = (y_predict-y) ** 2\n",
    "        mse = se.mean()\n",
    "        rmse = mse**0.5\n",
    "        return rmse\n",
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "my_model = Model()\n",
    "my_model.fit(X=X_train, y=y_train, alpha=0.01)\n",
    "rmse = my_model.rmse(X=X_test, y=y_test)\n",
    "print(f\"Root Mean Squre Error Ridge {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error 1 Layer MLP 13.11371619563616\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# Mean Squared Error loss\n",
    "def mse_loss(y_true, y_pred): \n",
    "    mse_err = np.sum(np.square(y_true - y_pred))/len(y_true)\n",
    "    return mse_err\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_o, b_o):\n",
    "    # The output of the hidden layer\n",
    "    a_h1 = np.dot(x, W_h1) + b_h1\n",
    "    # Apply activation to the output of the hidden layer\n",
    "    z_h1 = sigmoid(a_h1)\n",
    "    # Use ouput of activation as input to the output layer (it is just similar to first layer but we don't apply activation)\n",
    "    y_pred = np.dot(z_h1, W_o) + b_o\n",
    "    return y_pred, z_h1, a_h1\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, W_h1, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred\n",
    "    dL_dy_pred = 2 * (y_pred - y_true) / len(y_true)\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dL_dW_o = np.dot(z_h1.T, dL_dy_pred)\n",
    "    dL_db_o = np.sum(dL_dy_pred, axis = 0, keepdims=True)\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    dL_dz_h1 = np.dot(dL_dy_pred, W_o.T)\n",
    "    #TODO derivative with respect to the output of activation layer\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    dL_da_h1 = dL_dz_h1 * sigmoid_derivative(a_h1) # Derivative with respect to the activation layer\n",
    "    \n",
    "    # Gradients for hidden layer\n",
    "    dL_dW_h1 = np.dot(x.T, dL_da_h1)\n",
    "    dL_db_h1 = np.sum(dL_da_h1, axis=0,keepdims=True) \n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer_size = 20 # Number of neurons in layer\n",
    "output_size = 1 # predicted variable\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer_size))\n",
    "W_o = np.random.randn(hidden_layer_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "#To save the weights which give the lowest loss\n",
    "lowest_loss = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1 = forward_pass(X_train, W_h1, b_h1, W_o, b_o)\n",
    "    loss = mse_loss(y_train, y_pred)\n",
    "\n",
    "    if loss < lowest_loss:\n",
    "        lowest_loss = loss\n",
    "        # Save the best weights and biases\n",
    "        best_weights = (W_h1.copy(), b_h1.copy(), W_o.copy(), b_o.copy())\n",
    "\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, W_h1, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases\n",
    "    W_h1 -= learning_rate * dL_dW_h1\n",
    "    b_h1 -= learning_rate * dL_db_h1\n",
    "    W_o -= learning_rate * dL_dW_o\n",
    "    b_o -= learning_rate * dL_db_o\n",
    "\n",
    "\n",
    "W_h1_best, b_h1_best, W_o_best, b_o_best = best_weights\n",
    "y_pred_test, _, _ = forward_pass(X_test, W_h1_best, b_h1_best, W_o_best, b_o_best)\n",
    "se = (y_pred_test-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 1 Layer MLP {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Lets add one more hidden layer (10 Points) \n",
    "### You must write down all gradients and complete the code below to get full bonus points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_h2, b_h2, W_o, b_o):\n",
    "    a_h1 = #TODO compute ouput of first hidden layer\n",
    "    z_h1 = sigmoid(a_h1) # apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    a_h2 = #TODO compute ouput of second hidden layer\n",
    "    z_h2 = sigmoid(a_h2) #apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    y_pred = # compute ouput of output layer, why don't we apply activation ?\n",
    "    \n",
    "    return y_pred, z_h1, a_h1, z_h2, a_h2\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred, what kind of loss are we using ?\n",
    "    dL_dy_pred = #TODO\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dL_dW_o = #TODO\n",
    "    dL_db_o = #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h2\n",
    "    dL_dz_h2 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to a_h2\n",
    "    dL_da_h2 =  #TODO\n",
    "    \n",
    "    # Gradients for second hidden layer\n",
    "    dL_dW_h2 =  #TODO\n",
    "    dL_db_h2 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    dL_dz_h1 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    dL_da_h1 =  #TODO\n",
    "    \n",
    "    # Gradients for first hidden layer\n",
    "    dL_dW_h1 =  #TODO\n",
    "    dL_db_h1 =  #TODO\n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "# Random input and true output (modify these according to your dataset)\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer1_size = 100\n",
    "hidden_layer2_size = 20\n",
    "output_size = 1\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer1_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer1_size))\n",
    "W_h2 = np.random.randn(hidden_layer1_size, hidden_layer2_size) * 0.001\n",
    "b_h2 = np.zeros((1, hidden_layer2_size))\n",
    "W_o = np.random.randn(hidden_layer2_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Training loop\n",
    "for i in range(200):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1, z_h2, a_h2 = forward_pass(X_train, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "\n",
    "    #TODO Compute the loss\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases for each layer\n",
    "    \n",
    "    W_h1 -= #TODO\n",
    "    b_h1 -= #TODO\n",
    "    W_h2 -= #TODO\n",
    "    b_h2 -= #TODO\n",
    "    W_o -= #TODO\n",
    "    b_o -= #TODO\n",
    "\n",
    "y_pred, _, _, _, _ = forward_pass(X_test, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 2 Layer MLP {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform',  activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform', activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(400, kernel_initializer = 'he_uniform',activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "history = model.fit(X_train, y_train,  epochs = 200, validation_data=(X_test, y_test), \n",
    "                    callbacks = EarlyStopping(monitor = 'val_loss',patience = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE error using deeper neural network {model.evaluate(X_test, y_test)**0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
